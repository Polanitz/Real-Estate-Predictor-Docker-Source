{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\igaln\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\igaln\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\igaln\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\igaln\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\igaln\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\igaln\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "C:\\Users\\igaln\\anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\igaln\\anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\igaln\\anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\igaln\\anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\igaln\\anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\igaln\\anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\igaln\\anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      " * Serving Flask app \"__main__\" (lazy loading)\n",
      " * Environment: production\n",
      "   WARNING: This is a development server. Do not use it in a production deployment.\n",
      "   Use a production WSGI server instead.\n",
      " * Debug mode: on\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\igaln\\anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:143: FutureWarning: The sklearn.preprocessing.data module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.preprocessing. Anything that cannot be imported from sklearn.preprocessing is now part of the private API.\n",
      "  warnings.warn(message, FutureWarning)\n",
      "C:\\Users\\igaln\\anaconda3\\lib\\site-packages\\sklearn\\base.py:334: UserWarning: Trying to unpickle estimator StandardScaler from version 0.21.3 when using version 0.23.1. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  UserWarning)\n",
      "C:\\Users\\igaln\\anaconda3\\lib\\site-packages\\sklearn\\base.py:334: UserWarning: Trying to unpickle estimator OneHotEncoder from version 0.21.3 when using version 0.23.1. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  UserWarning)\n",
      "C:\\Users\\igaln\\anaconda3\\lib\\site-packages\\sklearn\\base.py:334: UserWarning: Trying to unpickle estimator ColumnTransformer from version 0.21.3 when using version 0.23.1. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  UserWarning)\n",
      " * Restarting with windowsapi reloader\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "1",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[1;31mSystemExit\u001b[0m\u001b[1;31m:\u001b[0m 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\igaln\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3339: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "# Import required python libraries\n",
    "from flask import Flask, render_template, request\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import traceback\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import make_column_transformer\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import Dense, Dropout\n",
    "# API definition\n",
    "app = Flask(__name__)\n",
    "\n",
    "@app.route('/')\n",
    "def home():\n",
    "    return render_template(\"frontend.html\")\n",
    "\n",
    "@app.route('/predict', methods=['POST'])\n",
    "def predict():\n",
    "    with graph.as_default():\n",
    "        payload = request.json\n",
    "        try:\n",
    "            model = load_model('pre_trained.hdf5')\n",
    "            with open('feature_transformer.sav', 'rb') as filehandle:\n",
    "                feature_transformer = pickle.load(filehandle)\n",
    "            with open('target_transformer.sav', 'rb') as filehandle:\n",
    "                target_transformer = pickle.load(filehandle)\n",
    "            BuildingArea = int(payload['BuildingArea'])\n",
    "            Rooms = int(payload['Rooms'])\n",
    "            Postcode = int(payload['Postcode'])\n",
    "            main_df = pd.read_csv('real_estate_data.csv')\n",
    "            if Postcode not in main_df.Postcode.unique():\n",
    "                return 'Data for Given Postal Code is not trained'\n",
    "            if Rooms not in main_df.Rooms.unique():\n",
    "                return 'Data for Given Number of Rooms is not trained'\n",
    "            if BuildingArea <= 0:\n",
    "                return 'Enter a valid build area'\n",
    "            input_array = np.array([[BuildingArea], [Rooms], [Postcode]]).reshape(1,-1)\n",
    "            df_temp = pd.DataFrame(input_array)\n",
    "            df_temp.columns = ['BuildingArea', 'Rooms', 'Postcode']\n",
    "            input_array = feature_transformer.transform(df_temp)\n",
    "            if model:\n",
    "                    prediction = target_transformer.inverse_transform(model.predict(input_array))\n",
    "                    return str(prediction[0][0])\n",
    "        except:\n",
    "            return str(traceback.format_exc())\n",
    "\n",
    "@app.route('/retrain', methods=['POST'])\n",
    "def retrain():\n",
    "    with graph.as_default():\n",
    "        payload = request.json\n",
    "        try:\n",
    "            BuildingArea = int(payload['BuildingArea'])\n",
    "            Rooms = int(payload['Rooms'])\n",
    "            Postcode = int(payload['Postcode'])\n",
    "            Price = int(payload['Price'])\n",
    "            if BuildingArea <= 0:\n",
    "                return 'Enter a valid build area'\n",
    "            input_array = np.array([[Rooms], [BuildingArea], [Postcode], [Price]]).reshape(1,-1)\n",
    "            df_temp = pd.DataFrame(input_array)\n",
    "            df_temp.columns = ['Rooms', 'BuildingArea', 'Postcode', 'Price']\n",
    "            main_df = pd.read_csv('real_estate_data.csv')\n",
    "            dataframe = pd.concat([main_df,df_temp]).reset_index(drop=True)\n",
    "            dataframe.dropna(inplace=True)\n",
    "            dataframe.to_csv('real_estate_data.csv', index=False)\n",
    "            feature_dataset = dataframe[['Rooms','BuildingArea','Postcode']]\n",
    "            target_dataset = dataframe[['Price']]\n",
    "            feature_transformer = make_column_transformer(\n",
    "                    (['BuildingArea'], StandardScaler()),\n",
    "                    (['Rooms', 'Postcode'], OneHotEncoder(categories=\"auto\",drop=\"first\"))\n",
    "            )\n",
    "            X = feature_transformer.fit_transform(feature_dataset)\n",
    "            with open('feature_transformer.sav', 'wb') as filehandle:\n",
    "                pickle.dump(feature_transformer, filehandle)\n",
    "            target_transformer = StandardScaler()\n",
    "            y = target_transformer.fit_transform(np.array(target_dataset).reshape(-1,1))            \n",
    "            with open('target_transformer.sav', 'wb') as filehandle:\n",
    "                pickle.dump(target_transformer, filehandle)\n",
    "            # define the keras model\n",
    "            model = Sequential()\n",
    "            # Layer 1\n",
    "            model.add(Dense(50, input_dim=X.shape[1], activation='relu'))\n",
    "            # Dropout regularization is added to avoid overfitting\n",
    "            model.add(Dropout(0.1))\n",
    "            # Layer 2\n",
    "            model.add(Dense(50, activation='relu'))\n",
    "            # Dropout regularization is added to avoid overfitting\n",
    "            model.add(Dropout(0.1))\n",
    "            # Layer 3\n",
    "            model.add(Dense(50, activation='relu'))\n",
    "            # Output Layer\n",
    "            model.add(Dense(1))\n",
    "            #Compile the model\n",
    "            model.compile(loss='mae', optimizer='adam', metrics=['mae'])\n",
    "            model.fit(X, y, batch_size=32, epochs=50)\n",
    "            model.save('pre_trained.hdf5')\n",
    "            \n",
    "            return \"Model retrained with new data\"\n",
    "        except:\n",
    "            return str(traceback.format_exc())\n",
    "\n",
    "@app.route('/reset', methods=['POST'])\n",
    "def reset():\n",
    "    with graph.as_default():\n",
    "        try:\n",
    "            dataframe = pd.read_csv('real_estate_data_org.csv')\n",
    "            dataframe.to_csv('real_estate_data.csv', index=False)\n",
    "            feature_dataset = dataframe.iloc[:, 0:3]\n",
    "            target_dataset = dataframe.iloc[:, 3]\n",
    "            feature_transformer = make_column_transformer(\n",
    "                    (['BuildingArea'], StandardScaler()),\n",
    "                    (['Rooms', 'Postcode'], OneHotEncoder(categories=\"auto\",drop=\"first\"))\n",
    "            )\n",
    "            X = feature_transformer.fit_transform(feature_dataset)\n",
    "            with open('feature_transformer.sav', 'wb') as filehandle:\n",
    "                pickle.dump(feature_transformer, filehandle)\n",
    "            target_transformer = StandardScaler()\n",
    "            y = target_transformer.fit_transform(np.array(target_dataset).reshape(-1,1))            \n",
    "            with open('target_transformer.sav', 'wb') as filehandle:\n",
    "                pickle.dump(target_transformer, filehandle)\n",
    "            # define the keras model\n",
    "            model = Sequential()\n",
    "            # Layer 1\n",
    "            model.add(Dense(50, input_dim=X.shape[1], activation='relu'))\n",
    "            # Dropout regularization is added to avoid overfitting\n",
    "            model.add(Dropout(0.1))\n",
    "            # Layer 2\n",
    "            model.add(Dense(50, activation='relu'))\n",
    "            # Dropout regularization is added to avoid overfitting\n",
    "            model.add(Dropout(0.1))\n",
    "            # Layer 3\n",
    "            model.add(Dense(50, activation='relu'))\n",
    "            # Output Layer\n",
    "            model.add(Dense(1))\n",
    "            #Compile the model\n",
    "            model.compile(loss='mae', optimizer='adam', metrics=['mae'])\n",
    "            model.fit(X, y, batch_size=32, epochs=50)\n",
    "            model.save('pre_trained.hdf5')\n",
    "            \n",
    "            return \"Model reset with original train data\"\n",
    "        except:\n",
    "            return str(traceback.format_exc())\n",
    "        \n",
    "if __name__ == '__main__':\n",
    "    global graph\n",
    "    graph = tf.get_default_graph()\n",
    "    global model \n",
    "    model = load_model('pre_trained.hdf5')\n",
    "    with open('feature_transformer.sav', 'rb') as filehandle:\n",
    "        global feature_transformer \n",
    "        feature_transformer = pickle.load(filehandle)\n",
    "    with open('target_transformer.sav', 'rb') as filehandle:\n",
    "        global target_transformer \n",
    "        target_transformer = pickle.load(filehandle)\n",
    "    app.run(host='0.0.0.0', port=5802, debug=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
